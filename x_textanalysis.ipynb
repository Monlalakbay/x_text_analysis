{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9561c-1254-4b28-865b-34fa31eaba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data handling and computation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "import gc\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Text processing and NLP\n",
    "import re\n",
    "import spacy\n",
    "import emoji\n",
    "from spellchecker import SpellChecker\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Topic modeling\n",
    "from bertopic import BERTopic\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Configure settings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas() \n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79961aaf-53e3-4a79-add9-59600e8132e5",
   "metadata": {},
   "source": [
    "# I. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef42775-324e-452e-acd1-3b4762818749",
   "metadata": {},
   "source": [
    "Data was mined using Instant Data Transfer on the evening of March 5, 2025. Once an X account was set-up, the top 20 recommnded travel accounts were followed and then the latest 10,000+ tweets were collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af1790-8fcb-479f-81cb-c02fbc7bf1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "df = pd.read_csv('x.csv')\n",
    "\n",
    "# Display basic info\n",
    "#df.info()\n",
    "\n",
    "# Verify the data\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422696c0-d912-405d-bd65-bea4c9a09b1e",
   "metadata": {},
   "source": [
    "# II. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad2a9f-f2f9-4e5b-8b67-75ca5e78fd8a",
   "metadata": {},
   "source": [
    "Given that the data was not done thru X official data scraping, some features needed to be dropped and pre-processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efbad3-662c-4a0c-8ddb-cdda42af52a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep necessary columns\n",
    "columns_to_keep = [\n",
    "    \"css-175oi2r href\",\n",
    "    \"css-1jxf684\",\n",
    "    \"css-1jxf684 2\",\n",
    "    \"css-146c3p1 href\",\n",
    "    \"css-146c3p1\",\n",
    "    \"css-1jxf684 4\",\n",
    "    \"css-175oi2r href 4\",\n",
    "    \"css-9pa8cd src 2\",\n",
    "    \"css-1jxf684 5\",\n",
    "    \"css-146c3p1 2\",\n",
    "    \"css-146c3p1 href 2\",\n",
    "    \"css-175oi2r href 5\",\n",
    "    \"r-4qtqp9 src\",\n",
    "    \"css-1jxf684 10\",\n",
    "    \"css-1jxf684 11\",\n",
    "    \"css-1jxf684 12\",\n",
    "    \"css-1jxf684 14\"\n",
    "\n",
    "]\n",
    "\n",
    "df_filtered = df[columns_to_keep]\n",
    "\n",
    "# Map the kept columns\n",
    "column_mapping = {\n",
    "    \"css-175oi2r href\": \"Tweet URL\",\n",
    "    \"css-1jxf684\": \"Source Name\",\n",
    "    \"css-1jxf684 2\": \"Source Handle\",\n",
    "    \"css-146c3p1 href\": \"Tweet Status URL\",\n",
    "    \"css-146c3p1\": \"Tweet Time Ago\",\n",
    "    \"css-1jxf684 4\": \"Tweet Text\",\n",
    "    \"css-175oi2r href 4\": \"Media URL\",\n",
    "    \"css-9pa8cd src 2\": \"Image URL\",\n",
    "    \"css-1jxf684 5\": \"Headline\",\n",
    "    \"css-146c3p1 2\": \"Source URL\",\n",
    "    \"css-146c3p1 href 2\": \"Analytics URL\",\n",
    "    \"css-175oi2r href 5\": \"Emoji URL\",\n",
    "    \"r-4qtqp9 src\": \"External Link\",\n",
    "    \"css-1jxf684 10\": \"Replies\",\n",
    "    \"css-1jxf684 11\": \"Views\",\n",
    "    \"css-1jxf684 12\": \"Retweets\",\n",
    "    \"css-1jxf684 14\": \"Likes\"\n",
    "}\n",
    "\n",
    "# Keep necessary columns and create a copy\n",
    "df_filtered = df[columns_to_keep].copy()\n",
    "\n",
    "# Rename columns\n",
    "df_filtered.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Display updated column names\n",
    "#df_filtered.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59f01c-9b58-4492-a769-29b18dd8dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values \n",
    "df_filtered.isna().sum() \n",
    "sns.heatmap(df_filtered.isna(), yticklabels=False, cbar=False, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b2bb28-b7bb-466b-aff7-0f112634bc7b",
   "metadata": {},
   "source": [
    "## A. Tweet date posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d33341-7e5b-47be-bfb5-ba777f7b619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Tweet Time Ago to datetime64 and adjust it to the mining date (March 5, evening)\n",
    "def convert_tweet_time(tweet_time, reference_date=datetime(2025, 3, 6)):\n",
    "    \n",
    "    # Handle NaN/None values\n",
    "    if pd.isna(tweet_time):\n",
    "        return (None, \"NaN value\")\n",
    "    \n",
    "    # Convert to string and strip whitespace\n",
    "    tweet_time = str(tweet_time).strip()\n",
    "    \n",
    "    # Skip empty strings\n",
    "    if not tweet_time:\n",
    "        return (None, \"Empty string\")\n",
    "    \n",
    "    try:\n",
    "        # Handle relative times (seconds, minutes, hours)\n",
    "        if 's' in tweet_time:\n",
    "            seconds = int(tweet_time.replace('s', ''))\n",
    "            return (reference_date - timedelta(seconds=seconds), None)\n",
    "        elif 'm' in tweet_time:\n",
    "            minutes = int(tweet_time.replace('m', ''))\n",
    "            return (reference_date - timedelta(minutes=minutes), None)\n",
    "        elif 'h' in tweet_time:\n",
    "            hours = int(tweet_time.replace('h', ''))\n",
    "            return (reference_date - timedelta(hours=hours), None)\n",
    "        \n",
    "        # Handle \"Month Day, Year\" format (like \"Jul 31, 2024\")\n",
    "        if ',' in tweet_time:\n",
    "            try:\n",
    "                parsed_date = datetime.strptime(tweet_time, '%b %d, %Y')\n",
    "                return (parsed_date, None)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Handle \"Month Day\" format (like \"Mar 3\")\n",
    "        parts = tweet_time.split()\n",
    "        if len(parts) == 2 and len(parts[0]) == 3 and parts[0].isalpha():\n",
    "            month, day = parts\n",
    "            try:\n",
    "                date_str = f\"{day}-{month}-{reference_date.year}\"\n",
    "                parsed_date = datetime.strptime(date_str, '%d-%b-%Y')\n",
    "                if parsed_date <= reference_date:\n",
    "                    return (parsed_date, None)\n",
    "                else:\n",
    "                    date_str = f\"{day}-{month}-{reference_date.year-1}\"\n",
    "                    parsed_date = datetime.strptime(date_str, '%d-%b-%Y')\n",
    "                    return (parsed_date, None)\n",
    "            except ValueError as e:\n",
    "                return (None, f\"Month Day format error: {str(e)}\")\n",
    "        \n",
    "        # Handle \"Day-Month\" format (like \"4-Mar\")\n",
    "        elif '-' in tweet_time and len(tweet_time.split('-')) == 2:\n",
    "            day, month = tweet_time.split('-')\n",
    "            try:\n",
    "                date_str = f\"{day}-{month}-{reference_date.year}\"\n",
    "                parsed_date = datetime.strptime(date_str, '%d-%b-%Y')\n",
    "                if parsed_date <= reference_date:\n",
    "                    return (parsed_date, None)\n",
    "                else:\n",
    "                    date_str = f\"{day}-{month}-{reference_date.year-1}\"\n",
    "                    parsed_date = datetime.strptime(date_str, '%d-%b-%Y')\n",
    "                    return (parsed_date, None)\n",
    "            except ValueError as e:\n",
    "                return (None, f\"Day-Month format error: {str(e)}\")\n",
    "        \n",
    "        # Handle full dates (like \"3-Jan-23\")\n",
    "        elif '-' in tweet_time and len(tweet_time.split('-')) == 3:\n",
    "            try:\n",
    "                if len(tweet_time.split('-')[2]) == 2:\n",
    "                    parsed_date = datetime.strptime(tweet_time, '%d-%b-%y')\n",
    "                else:\n",
    "                    parsed_date = datetime.strptime(tweet_time, '%d-%b-%Y')\n",
    "                return (parsed_date, None)\n",
    "            except ValueError as e:\n",
    "                return (None, f\"Full date format error: {str(e)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return (None, f\"Unexpected error: {str(e)}\")\n",
    "    \n",
    "    # If format wasn't recognized\n",
    "    return (None, \"Unrecognized format\")\n",
    "\n",
    "# Apply conversion and track errors\n",
    "conversion_results = df_filtered['Tweet Time Ago'].apply(\n",
    "    lambda x: convert_tweet_time(x)\n",
    ")\n",
    "\n",
    "# Split results into separate columns\n",
    "df_filtered['Tweet Datetime Posted'] = conversion_results.apply(lambda x: x[0])\n",
    "df_filtered['conversion_error'] = conversion_results.apply(lambda x: x[1])  # Add this line to store errors\n",
    "\n",
    "# Show entries that couldn't be converted before deleting them\n",
    "failed_conversions = df_filtered[df_filtered['Tweet Datetime Posted'].isna()]\n",
    "print(f\"\\nFailed to convert {len(failed_conversions)} entries:\")\n",
    "print(failed_conversions[['Tweet Time Ago', 'conversion_error']].to_string())\n",
    "\n",
    "# Show conversion statistics\n",
    "success_count = len(df_filtered) - len(failed_conversions)\n",
    "print(f\"\\nConversion Summary:\")\n",
    "print(f\"Successfully converted: {success_count}\")\n",
    "print(f\"Failed to convert: {len(failed_conversions)}\")\n",
    "print(f\"Success rate: {success_count/len(df_filtered):.1%}\")\n",
    "\n",
    "# Keep only rows where conversion was successful\n",
    "df_filtered = df_filtered[df_filtered['Tweet Datetime Posted'].notna()]\n",
    "\n",
    "# Verify the deletion\n",
    "print(f\"\\nAfter deletion, dataframe has {len(df_filtered)} rows\")\n",
    "\n",
    "# Show sample of successful conversions\n",
    "#print(\"\\nSample of successful conversions:\")\n",
    "#print(df_filtered[['Tweet Time Ago', 'Tweet Datetime Posted']].head(10).to_string())\n",
    "\n",
    "df_filtered.drop(['Tweet Time Ago', 'conversion_error'], axis=1, inplace=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d3889-5e16-424f-9028-1e3d60dc9553",
   "metadata": {},
   "source": [
    "## B. Engagement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdcf926-c487-410e-b94a-cfb5c7767a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts X engagement metrics' number formats (1.5K, 2,000)\n",
    "for col in ['Replies', 'Views', 'Retweets', 'Likes']:\n",
    "    df_filtered[col] = (\n",
    "        df_filtered[col]\n",
    "        .astype(str)\n",
    "        .str.replace(',', '')\n",
    "        .str.replace('K', 'e3', case=False)\n",
    "        .str.replace('M', 'e6', case=False)\n",
    "    )\n",
    "    df_filtered[col] = (\n",
    "        pd.to_numeric(df_filtered[col], errors='coerce')\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "# Spot-check random rows\n",
    "#print(df_filtered[col].sample(5))          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf22431-f7ac-404d-976a-3332bc90f62d",
   "metadata": {},
   "source": [
    "## C. Duplicate rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389dfa06-5df6-4883-8fbe-9d1caef88441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete duplicate rows \n",
    "df_filtered = df_filtered.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610cd870-07d3-4d95-a670-f378d3af9dfd",
   "metadata": {},
   "source": [
    "## D. Tweet text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38285cfb-17c0-4469-883c-45c90ded0f74",
   "metadata": {},
   "source": [
    "### 1. Special cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ca638-0f82-47ca-b4f5-895b17b4e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count entires with hashtags\n",
    "hashtag_count = df_filtered[\"Tweet Text\"].str.contains(r\"#\\w+\", regex=True, na=False).sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of tweets with hashtags: {hashtag_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e532d71c-4a50-49d6-acbc-104e8e9ca334",
   "metadata": {},
   "source": [
    "### 2. Lowercase, special cases & spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fccea29-35f4-4371-afed-7c5cb5cb29c1",
   "metadata": {},
   "source": [
    "Very few tweets have hashtags, proceeds to delete them along with URLs, mentions (@usernames), emojis, and special characters. Spelling is checked using Pyspellchecker, because it is lightweight (No need for external dictionaries like SymSpell) and works well for American & British English spelling. It doesn’t correct based on context like TextBlob, but is useful enough for correcting individual words in short text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186129c3-7d03-4960-970c-84aaba4675a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable progress bars for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Clean tweet text\n",
    "def clean_tweet(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()                       # Convert to lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)     # Remove URLs\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)          # Remove mentions & hashtags\n",
    "    text = emoji.replace_emoji(text, replace=\"\")   # Remove emojis\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)        # Remove special characters\n",
    "    return text.strip()\n",
    "\n",
    "# Efficient spelling correction using cached spellchecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "@lru_cache(maxsize=20000)  # Cache corrections to avoid redundant lookups\n",
    "def get_correction(word):\n",
    "    return word if word in spell else spell.correction(word) or word\n",
    "\n",
    "def correct_spelling(text):\n",
    "    words = text.split()\n",
    "    corrected = [get_correction(word) for word in words]\n",
    "    return \" \".join(corrected)\n",
    "\n",
    "# Apply cleaning and correction with progress bars\n",
    "df_filtered[\"Cleaned Tweet\"] = df_filtered[\"Tweet Text\"].progress_apply(clean_tweet)\n",
    "df_filtered[\"Cleaned Tweet\"] = df_filtered[\"Cleaned Tweet\"].progress_apply(correct_spelling)\n",
    "\n",
    "# Drop the original column\n",
    "df_filtered.drop(columns=[\"Tweet Text\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91d4b4-d07e-498d-b194-1d8d133aefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display pre-procecessed dataFrame\n",
    "df_filtered.info()\n",
    "#df_filtered.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fdd652-06de-4b11-b234-69bf245119ce",
   "metadata": {},
   "source": [
    "# III. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4233703a-f725-4639-9863-2475a48d56b9",
   "metadata": {},
   "source": [
    "## A. Entity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e07cb02-0ca9-4bfe-809a-5cbb39ba8e8d",
   "metadata": {},
   "source": [
    "### 1. Hashtag "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd124dde-313b-4812-9817-71fc90136f10",
   "metadata": {},
   "source": [
    "Only 4 tweets have hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a5cd2-7ece-4a10-a40c-ab1593de8695",
   "metadata": {},
   "source": [
    "### 2. Media & links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b029de-2524-4fea-bc0d-b11d0088d13d",
   "metadata": {},
   "source": [
    "The majority of tweets that do not include any media, images, or external links still received high views and engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965d476-201a-4b27-9da0-5e31bf25f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flags for presence of media, image, and external links\n",
    "df_filtered['has_media'] = df_filtered['Media URL'].notnull()\n",
    "df_filtered['has_image'] = df_filtered['Image URL'].notnull()\n",
    "df_filtered['has_link'] = df_filtered['External Link'].notnull()\n",
    "\n",
    "# Grouped averages\n",
    "media_stats = df_filtered.groupby('has_media')[['Replies', 'Views', 'Retweets', 'Likes']].mean().reset_index()\n",
    "image_stats = df_filtered.groupby('has_image')[['Replies', 'Views', 'Retweets', 'Likes']].mean().reset_index()\n",
    "link_stats = df_filtered.groupby('has_link')[['Replies', 'Views', 'Retweets', 'Likes']].mean().reset_index()\n",
    "\n",
    "# Visualization\n",
    "def plot_engagement(df, col, title):\n",
    "    df_melted = df.melt(id_vars=col, var_name='Engagement Type', value_name='Average Count')\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(data=df_melted, x='Engagement Type', y='Average Count', hue=col)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Average Engagement')\n",
    "    plt.xlabel('')\n",
    "    plt.legend(title=col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_engagement(media_stats, 'has_media', 'Impact of Media with Engagement')\n",
    "plot_engagement(image_stats, 'has_image', 'Impact of Image with Engagement')\n",
    "plot_engagement(link_stats, 'has_link', 'Impact of External Link with Engagement ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf87f1e3-2eec-4ed7-940f-bc11d60127c2",
   "metadata": {},
   "source": [
    "### 3. Active users (based on number of Tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec56eb29-604f-4cc9-9d90-e190b0efbda1",
   "metadata": {},
   "source": [
    "Most handles belong to well-known travel media outlets (e.g., @CNTraveler, @TelegraphTravel, @TravelWeeklyUS, @NatGeoTravelLIK)  indicating that professional travel publishers are highly active on Twitter. These accounts may prioritize frequent updates to stay relevant or cover breaking travel news. \n",
    "\n",
    "The presence of travel guides/review platforms like (e.g., @Tripadvisor and @frommers) tweet less frequently than media outlets, possibly focusing on curated content over volume.\n",
    "\n",
    "The absence of individual influencers suggests that institutions dominate high-volume tweeting in the travel niche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e864ced6-eae7-4800-9b56-a28d87d67835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 most active users by tweet count \n",
    "top_active = df_filtered['Source Handle'].value_counts().head(10)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_active.values, y=top_active.index, palette='Blues_r')\n",
    "plt.title('Top 10 Most Active Users (by Tweet Count)')\n",
    "plt.xlabel('Number of Tweets')\n",
    "plt.ylabel('Source Handle')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819a61f5-5a8f-4647-9cb0-a16f59ae90a4",
   "metadata": {},
   "source": [
    "### 4. Most engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770dec0d-a515-431a-9f93-8094c841b2e8",
   "metadata": {},
   "source": [
    "Visibility vs. Engagement: High tweet counts don't necessarily correlate with high engagement.\n",
    "\n",
    "Media Brands (e.g., @Cntraveller, @usatodaytravel) focus on tweet volume and replies, but lack viral reach.\n",
    "\n",
    "Individual Influencers (e.g., @Rainmaker1973, @TravelAndLove) excel in passive engagement (views/likes) and shares, which may mean that their content is optimized for algorithms.\n",
    "\n",
    "@PeteButtigieg (a public figure) leads in Replies and is in the top 10 of other metrics, shows that his account drives discussions but may not attract casual viewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e75c5-2ebf-4076-8b10-3643d7a023e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 users by total engagement metrics\n",
    "# Grouping and sorting\n",
    "engagement_by_user = df_filtered.groupby('Source Handle')[['Replies', 'Views', 'Retweets', 'Likes']].sum()\n",
    "top_replied_users = engagement_by_user.sort_values(by='Replies', ascending=False).head(10)\n",
    "top_viewed_users = engagement_by_user.sort_values(by='Views', ascending=False).head(10)\n",
    "top_liked_users = engagement_by_user.sort_values(by='Likes', ascending=False).head(10)\n",
    "top_retweeted_users = engagement_by_user.sort_values(by='Retweets', ascending=False).head(10)\n",
    "\n",
    "# Visualization\n",
    "def plot_top_users(data, metric, color, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=data[metric].values, y=data.index, palette=color)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(f'Total {metric}')\n",
    "    plt.ylabel('Source Handle')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_top_users(top_replied_users, 'Replies', 'Blues_r', 'Top 10 Users by Total Replies')\n",
    "plot_top_users(top_viewed_users, 'Views', 'Greens_r', 'Top 10 Users by Total Views')\n",
    "plot_top_users(top_liked_users, 'Likes', 'Reds_r', 'Top 10 Users by Total Likes')\n",
    "plot_top_users(top_retweeted_users, 'Retweets', 'Purples_r', 'Top 10 Users by Total Retweets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c59cc4-6d55-4bfb-b345-cff37e1c0a94",
   "metadata": {},
   "source": [
    "### 5. Per-tweet engagement averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859d10b-d182-4cc5-a732-e16d4c74a9e2",
   "metadata": {},
   "source": [
    "Quality over Quantity: Accounts like @Rainmaker1973 and @PeteButtigieg prove that fewer, high-impact tweets outperform volume-driven strategies by traditional travel media.\n",
    "\n",
    "\n",
    "Virality (e.g., @Rainmaker1973) win views.\n",
    "\n",
    "Debatable Topics (e.g., @PeteButtigieg) drive replies.\n",
    "\n",
    "Inspiration (e.g., @TravelAndLove) earns likes/shares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872fad2c-9805-4d26-bb56-ecc1567f0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 users by average engagement metrics\n",
    "# Calculate total tweets per user\n",
    "tweets_per_user = df_filtered.groupby('Source Handle').size()\n",
    "\n",
    "# Calculate per-tweet averages for each user\n",
    "engagement_per_tweet = engagement_by_user.div(tweets_per_user, axis=0)\n",
    "\n",
    "# Get top 10 users by per-tweet likes, retweets, and views\n",
    "top_per_tweet_replies = engagement_per_tweet.sort_values(by='Replies', ascending=False).head(10)\n",
    "top_per_tweet_views = engagement_per_tweet.sort_values(by='Views', ascending=False).head(10)\n",
    "top_per_tweet_likes = engagement_per_tweet.sort_values(by='Likes', ascending=False).head(10)\n",
    "top_per_tweet_retweets = engagement_per_tweet.sort_values(by='Retweets', ascending=False).head(10)\n",
    "\n",
    "# Visualization\n",
    "def plot_per_tweet_engagement(data, metric, color, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=data[metric].values, y=data.index, palette=color)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(f'Average {metric} per Tweet')\n",
    "    plt.ylabel('Source Handle')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_per_tweet_engagement(top_per_tweet_replies, 'Replies', 'Blues_r', 'Top 10 Users by Average Replies per Tweet')\n",
    "plot_per_tweet_engagement(top_per_tweet_views, 'Views', 'Greens_r', 'Top 10 Users by Average Views per Tweet')\n",
    "plot_per_tweet_engagement(top_per_tweet_likes, 'Likes', 'Reds_r', 'Top 10 Users by Average Likes per Tweet')\n",
    "plot_per_tweet_engagement(top_per_tweet_retweets, 'Retweets', 'Purples_r', 'Top 10 Users by Average Retweets per Tweet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688549c-4753-4a41-a767-6c44432ca8aa",
   "metadata": {},
   "source": [
    "## B. LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d3a6a-7a8c-4b31-8437-3d9abf3ca0b5",
   "metadata": {},
   "source": [
    "LDA assumes that topics are composed of multiple words. Separating these preprocessing steps allowed for finer control over what data was included.\n",
    "\n",
    "For example, specific travel-related stop words were added uniquely for this analysis, because it led to more accurate and meaningful topics from the short tweets.\n",
    "\n",
    "Memory usage was also improved in iterations, as processing a large dataset required experimenting with ways of optimizing the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df96301-66c1-42ee-8897-cfe6fb95fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LDA Topic Modeling Pipeline\n",
    "\n",
    "This script performs LDA topic modeling on travel-related tweets, including:\n",
    "- Text preprocessing (tokenization, lemmatization, stopword removal)\n",
    "- Dictionary and corpus creation\n",
    "- LDA model training\n",
    "- Visualization and analysis\n",
    "- Trend analysis over time\n",
    "\n",
    "Key Components:\n",
    "1. Text preprocessing with spaCy\n",
    "2. Gensim for LDA implementation\n",
    "3. pyLDAvis for interactive visualization\n",
    "4. Plotly for trend visualizations\n",
    "\"\"\"\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import logging\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Initialize spaCy with optimized settings\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "# Define travel-specific parameters\n",
    "TRAVEL_KEEP_POS = {\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\"}  # Parts of speech to keep\n",
    "\n",
    "# Custom stopwords for travel domain:\n",
    "# Tweaked based on previous iterations,it lead more accurate and meaningful topics from the short tweets.\n",
    "TRAVEL_STOPWORDS = {  \n",
    "    'travel', 'trip', 'vacation', 'journey', 'go', 'visit', 'see', 'book', 'fly',\n",
    "    'here', 'know', 'like', 'get', 'one', 'time', 'make', 'take', 'want'\n",
    "}\n",
    "\n",
    "# Setup logging for better tracking and error handling\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def process_travel_tweets(df, text_col=\"Cleaned Tweet\", max_docs=None):\n",
    "    \"\"\"\n",
    "    Roboust method to clean and process travel tweets with optimized memory management.\n",
    "    \n",
    "    Performs:\n",
    "    - Tokenization using spaCy\n",
    "    - Lemmatization\n",
    "    - POS filtering\n",
    "    - Custom stopword removal\n",
    "    - Length filtering\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe containing previously pre-processed tweets\n",
    "        text_col (str): Column name containing text to process\n",
    "        max_docs (int): Maximum number of documents to process (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added 'Processed_Tokens' column\n",
    "    \n",
    "    Customization Points:\n",
    "        - TRAVEL_KEEP_POS: Change POS tags to keep\n",
    "        - TRAVEL_STOPWORDS: Add/remove domain-specific stopwords\n",
    "        - max_docs: Adjust for memory constraints\n",
    "    \"\"\"\n",
    "    if max_docs:\n",
    "        df = df.iloc[:max_docs].copy()\n",
    "    \n",
    "    texts = df[text_col].astype(str).tolist()\n",
    "    \n",
    "    # Process texts in batches for memory efficiency\n",
    "    processed_texts = []\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=1000), total=len(texts), desc=\"Processing tweets\"):\n",
    "        tokens = [\n",
    "            token.lemma_.lower() for token in doc\n",
    "            if (token.pos_ in TRAVEL_KEEP_POS and \n",
    "                token.lemma_.lower() not in TRAVEL_STOPWORDS and\n",
    "                len(token.lemma_) > 2)\n",
    "        ]\n",
    "        processed_texts.append(tokens)\n",
    "    \n",
    "    df['Processed_Tokens'] = processed_texts\n",
    "    return df\n",
    "\n",
    "def train_lda_model(df, num_topics=5, max_docs=None):\n",
    "    \"\"\"\n",
    "    Train LDA model on processed tweets with memory-efficient settings.\n",
    "    \n",
    "    Steps:\n",
    "    1. Creates dictionary from processed tokens\n",
    "    2. Filters extreme values\n",
    "    3. Creates bag-of-words corpus\n",
    "    4. Trains LDA model\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Processed dataframe from process_travel_tweets()\n",
    "        num_topics (int): Number of topics to extract\n",
    "        max_docs (int): Maximum documents to use (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (lda_model, corpus, id2word dictionary)\n",
    "    \n",
    "    Tunable Parameters:\n",
    "        - num_topics: Ideal range 3-15 depending on dataset size\n",
    "        - no_below/no_above: Dictionary filtering thresholds\n",
    "        - chunksize: Adjust based on available RAM\n",
    "        - passes/iterations: Control training duration/quality\n",
    "    \"\"\"\n",
    "    if max_docs:\n",
    "        df = df.iloc[:max_docs].copy()\n",
    "    \n",
    "    # Create and filter dictionary\n",
    "    id2word = corpora.Dictionary(df['Processed_Tokens'])\n",
    "    id2word.filter_extremes(no_below=5, no_above=0.5)\n",
    "    logger.info(f\"Dictionary size: {len(id2word)}\")\n",
    "    \n",
    "    # Create corpus as list \n",
    "    corpus = [id2word.doc2bow(tokens) for tokens in df['Processed_Tokens']]\n",
    "    \n",
    "    # Train LDA with memory-efficient settings\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=num_topics,\n",
    "        chunksize=1000,\n",
    "        passes=2,\n",
    "        iterations=50,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        eval_every=None,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return lda_model, corpus, id2word\n",
    "\n",
    "def visualize_topics(lda_model, corpus, id2word):\n",
    "    \"\"\"\n",
    "    Generate interactive pyLDAvis visualization of topics.\n",
    "    \n",
    "    Args:\n",
    "        lda_model: Trained LDA model\n",
    "        corpus: Processed bag-of-words corpus\n",
    "        id2word: Gensim dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Prepared pyLDAvis visualization object\n",
    "    \"\"\"\n",
    "    vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    pyLDAvis.display(vis)\n",
    "    return vis\n",
    "\n",
    "def visualize_topics_enhanced(lda_model, num_words=10):\n",
    "    \"\"\"\n",
    "    Create enhanced visualizations of topics including:\n",
    "    - DataFrame of top terms\n",
    "    - Horizontal bar charts\n",
    "    - Styled table output\n",
    "    \n",
    "    Args:\n",
    "        lda_model: Trained LDA model\n",
    "        num_words: Number of top words to show per topic\n",
    "    \"\"\"\n",
    "    # Create DataFrame of topic words\n",
    "    topic_words = []\n",
    "    for topic_id in range(lda_model.num_topics):\n",
    "        words = lda_model.show_topic(topic_id, topn=num_words)\n",
    "        topic_words.append([word[0] for word in words])\n",
    "    \n",
    "    topic_df = pd.DataFrame(topic_words).T\n",
    "    topic_df.columns = [f'Topic {i}' for i in range(lda_model.num_topics)]\n",
    "    \n",
    "    # Print table\n",
    "    print(\"\\n=== Top Terms Per Topic ===\")\n",
    "    display(topic_df.style.background_gradient(cmap='Blues'))\n",
    "    \n",
    "    # Create bar charts for each topic\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(lda_model.num_topics):\n",
    "        plt.subplot(3, 2, i+1)\n",
    "        topic = lda_model.show_topic(i, num_words)\n",
    "        words = [word[0] for word in topic]\n",
    "        weights = [word[1] for word in topic]\n",
    "        \n",
    "        plt.barh(words, weights, color='skyblue')\n",
    "        plt.title(f'Topic {i} - Dominant Terms')\n",
    "        plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_topic_samples(df, lda_model, id2word, n_samples=3):\n",
    "    \"\"\"\n",
    "    Get representative sample tweets for each topic.\n",
    "    \n",
    "    Args:\n",
    "        df: Processed DataFrame with tweets\n",
    "        lda_model: Trained LDA model\n",
    "        id2word: Gensim dictionary\n",
    "        n_samples: Number of samples per topic\n",
    "    \n",
    "    Returns:\n",
    "        dict: {topic_id: [sample_tweets]}\n",
    "    \"\"\"\n",
    "    \n",
    "    df['topic_probs'] = df['Processed_Tokens'].apply(\n",
    "        lambda x: dict(lda_model.get_document_topics(id2word.doc2bow(x))))\n",
    "    \n",
    "    df['dominant_topic'] = df['topic_probs'].apply(\n",
    "        lambda x: max(x.items(), key=lambda item: item[1])[0] if x else -1)\n",
    "    \n",
    "    # Get samples for each topic\n",
    "    samples = {}\n",
    "    for topic in range(lda_model.num_topics):\n",
    "        topic_tweets = df[df['dominant_topic'] == topic]\n",
    "        samples[topic] = topic_tweets.sample(\n",
    "            min(n_samples, len(topic_tweets)))['Cleaned Tweet'].tolist()\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def analyze_topic_trends(df, lda_model, id2word):\n",
    "    \"\"\"\n",
    "    Analyze topic trends over time with memory-efficient processing.\n",
    "    \n",
    "    Args:\n",
    "        df: Processed DataFrame with datetime info\n",
    "        lda_model: Trained LDA model\n",
    "        id2word: Gensim dictionary\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Topic trends with columns [date, topic_id, probability]\n",
    "    \"\"\"\n",
    "    topic_data = []\n",
    "    \n",
    "    # Process in chunks for memory efficiency\n",
    "    chunk_size = 1000\n",
    "    for i in tqdm(range(0, len(df), chunk_size), desc=\"Analyzing topics\"):\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        for _, row in chunk.iterrows():\n",
    "            bow = id2word.doc2bow(row['Processed_Tokens'])\n",
    "            topic_dist = lda_model.get_document_topics(bow)\n",
    "            for topic_id, prob in topic_dist:\n",
    "                topic_data.append({\n",
    "                    'date': row['Tweet Datetime Posted'].date(),\n",
    "                    'topic_id': topic_id,\n",
    "                    'probability': prob\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(topic_data)\n",
    "\n",
    "def visualize_topic_trends(trend_df):\n",
    "    \"\"\"\n",
    "    Visualize topic trends over time using Plotly.\n",
    "    \n",
    "    Args:\n",
    "        trend_df: DataFrame from analyze_topic_trends()\n",
    "    \n",
    "    Returns:\n",
    "        plotly Figure object\n",
    "    \"\"\"\n",
    "    # Aggregate probabilities by date and topic\n",
    "    trend_agg = trend_df.groupby(['date', 'topic_id'])['probability'].mean().reset_index()\n",
    "    \n",
    "    # Create interactive plot\n",
    "    fig = px.line(trend_agg, \n",
    "                 x='date', \n",
    "                 y='probability',\n",
    "                 color='topic_id',\n",
    "                 title='Topic Trends Over Time',\n",
    "                 labels={'probability': 'Average Topic Probability', 'date': 'Date'},\n",
    "                 template='plotly_white')\n",
    "    \n",
    "    fig.update_layout(hovermode='x unified')\n",
    "    return fig\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main execution pipeline for travel tweet topic modeling.\n",
    "    \n",
    "    Steps:\n",
    "    1. Preprocess tweets\n",
    "    2. Train LDA model\n",
    "    3. Generate visualizations\n",
    "    4. Save results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\n=== LDA ANALYSIS PIPELINE ===\")\n",
    "        # Process tweets in batches\n",
    "        df_processed = process_travel_tweets(df_filtered, max_docs=11000)\n",
    "        \n",
    "        # Train LDA model\n",
    "        lda_model, corpus, id2word = train_lda_model(df_processed, num_topics=5)\n",
    "        \n",
    "        # Print topic words\n",
    "        print(\"\\n=== Topic Word Distributions ===\")\n",
    "        for idx, topic in lda_model.print_topics(-1):\n",
    "            print(f\"\\nTopic {idx}:\\n{topic}\")\n",
    "        \n",
    "        # Create enhanced visualizations\n",
    "        visualize_topics_enhanced(lda_model)\n",
    "        \n",
    "        # Generate interactive visualization\n",
    "        print(\"\\nGenerating interactive topic visualization...\")\n",
    "        vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "        pyLDAvis.save_html(vis, 'lda_visualization.html')\n",
    "        print(\"Saved pyLDAvis to 'lda_visualization.html'\")\n",
    "        \n",
    "        # Get sample tweets per topic\n",
    "        topic_samples = get_topic_samples(df_processed, lda_model, id2word)\n",
    "        print(\"\\n=== Sample Tweets for Each Topic ===\")\n",
    "        for topic, samples in topic_samples.items():\n",
    "            print(f\"\\nTopic {topic} - Characteristic Tweets:\")\n",
    "            for i, tweet in enumerate(samples, 1):\n",
    "                print(f\"{i}. {tweet}\")\n",
    "\n",
    "        # Analyze and visualize topic trends\n",
    "        print(\"\\nAnalyzing topic trends over time...\")\n",
    "        topic_trends = analyze_topic_trends(df_processed, lda_model, id2word)\n",
    "        trend_fig = visualize_topic_trends(topic_trends)\n",
    "        \n",
    "        # Save trend visualization\n",
    "        trend_fig.write_html(\"lda_topic_trends_over_time.html\")\n",
    "        print(\"Saved topic trends visualization to 'lda_topic_trends_over_time.html'\")\n",
    "                \n",
    "        # Save comprehensive results\n",
    "        print(\"\\nSaving comprehensive LDA results to CSV...\")\n",
    "        output_df = df_processed.copy()\n",
    "        output_df['topic_prob'] = output_df.apply(\n",
    "            lambda row: row['topic_probs'].get(row['dominant_topic'], 0), axis=1)\n",
    "        \n",
    "        for topic_id in range(lda_model.num_topics):\n",
    "            output_df[f\"topic_{topic_id}_prob\"] = df_processed['topic_probs'].apply(\n",
    "                lambda x: x.get(topic_id, 0))\n",
    "        \n",
    "        output_df.to_csv(\"lda_results.csv\", index=False)\n",
    "        print(\"Saved complete results to lda_results.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        \n",
    "    finally:\n",
    "        # Clean up large objects\n",
    "        if 'df_processed' in locals():\n",
    "            del df_processed\n",
    "        if 'corpus' in locals():\n",
    "            del corpus\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\nLDA analysis complete. Results saved to:\")\n",
    "    print(\"- lda_visualization.html (interactive topics)\")\n",
    "    print(\"- lda_results.csv (complete analysis)\")\n",
    "    print(\"- lda_topic_trends_over_time.html (trends)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508339ab-5597-47d7-aee0-0fa1eb6e6428",
   "metadata": {},
   "source": [
    "## C. BERTweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca873190-961e-4836-a59d-eb22dc7b4a32",
   "metadata": {},
   "source": [
    "Careful cleaning and preprocessing were essential with BERTopic and BERTweet embeddings. Aggressive preprocessing initially led to unproductive results, but after fine-tuning stop words and vectorizer settings, valuable insights emerged.\n",
    "\n",
    "Initially, 60+ topics were suggested with a good silhouette score, but these were reduced to under 20, prioritizing topic interpretability over strict clustering metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca150158-8b51-42f5-a516-a00f7874a676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177c8f9-5a8e-4cfa-860c-edcc2ef1db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BERTopic Analysis Pipeline \n",
    "\n",
    "This script performs advanced topic modeling using BERTopic, including:\n",
    "- Text cleaning and preprocessing\n",
    "- BERTweet embeddings generation\n",
    "- Dynamic topic modeling with BERTopic\n",
    "- Visualization\n",
    "- Trend analysis over time\n",
    "\n",
    "Key Components:\n",
    "1. Text preprocessing with spaCy\n",
    "2. BERTweet embeddings generation\n",
    "3. BERTopic for dynamic topic modeling\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import normalize\n",
    "from bertopic import BERTopic\n",
    "import pyLDAvis\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Setup logging for better tracking and error handling\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration\n",
    "MAX_BATCH_SIZE = 3000  # Max texts per preprocessing batch for memory efficiency\n",
    "EMBEDDING_BATCH_SIZE = 32  # Batch size for embedding generation\n",
    "\n",
    "# Initialize spaCy with optimized settings\n",
    "print(\"Loading NLP models...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\", \"tagger\"]) \n",
    "\n",
    "# Parts of speech to keep\n",
    "TRAVEL_KEEP_POS = {\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\", \"ADV\", \"DET\", \"PRON\"}\n",
    "\n",
    "# Custom stopwords for travel domain, tweaked based on previous iterations\n",
    "TRAVEL_STOPWORDS = set([\n",
    "    \"the\", \"a\", \"an\", \"and\", \"in\", \"on\", \"for\", \"to\", \"with\", \"at\", \"by\", \"of\", \n",
    "    \"from\", \"as\", \"it\", \"is\", \"that\", \"this\", \"you\", \"are\", \"i\", \"be\", \"have\"\n",
    "])\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Roboust method to clean travel tweets.\n",
    "    \n",
    "    Performs:\n",
    "    - Removing URLs, mentions, hashtags\n",
    "    - Removing special characters\n",
    "    - Normalizing whitespace\n",
    "    - Converting to lowercase\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input containing previously pre-processed tweets. \n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\w+|#\\w+', '', text)  # Remove URLs/mentions\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove special chars\n",
    "    return re.sub(r'\\s+', ' ', text).strip().lower()  # Normalize whitespace\n",
    "\n",
    "def preprocess_batch(texts):\n",
    "    \"\"\"\n",
    "    Process batch of tweets with spaCy pipeline with memory-efficient settings.\n",
    "\n",
    "    Performs:\n",
    "    - Tokenization\n",
    "    - Lemmatization\n",
    "    - Stopword removal\n",
    "    - POS filtering\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of raw text strings\n",
    "        \n",
    "    Returns:\n",
    "        list: List of processed text strings\n",
    "    \"\"\"\n",
    "    processed_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=1000):\n",
    "        tokens = [token.lemma_.lower() for token in doc \n",
    "                 if token.lemma_.lower() not in TRAVEL_STOPWORDS]\n",
    "        processed_texts.append(\" \".join(tokens) if tokens else \"\")\n",
    "    return processed_texts\n",
    "\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"\n",
    "    BERTweet Embedding Generator.\n",
    "    \n",
    "    Performs:\n",
    "    - Model loading\n",
    "    - Batch processing\n",
    "    - GPU utilization\n",
    "    - Embedding normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize BERTweet model and tokenizer\"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"  • Using device: {self.device}\")\n",
    "        print(\"  • Loading BERTweet model...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "        self.model = AutoModel.from_pretrained(\"vinai/bertweet-base\").to(self.device).eval()\n",
    "        print(\"  • Model loaded successfully\")\n",
    "\n",
    "    def generate_embeddings(self, texts):\n",
    "        \"\"\"\n",
    "        Generate BERTweet embeddings for input texts\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of preprocessed text strings\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Normalized embeddings matrix\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        logging.info(f\"  • Generating embeddings for {len(texts)} texts...\")\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), EMBEDDING_BATCH_SIZE), \n",
    "                    desc=\"Generating Embeddings\", unit=\"batch\"):\n",
    "            batch = texts[i:i + EMBEDDING_BATCH_SIZE]\n",
    "            with torch.no_grad():\n",
    "                inputs = self.tokenizer(\n",
    "                    batch, \n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    max_length=64\n",
    "                ).to(self.device)\n",
    "                outputs = self.model(**inputs)\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.append(batch_embeddings)\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if not embeddings:\n",
    "            return np.array([])\n",
    "        \n",
    "        # Stack all embeddings into a 2D array\n",
    "        all_embeddings = np.vstack(embeddings)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        if all_embeddings.size > 0:\n",
    "            try:\n",
    "                print(\"  • Normalizing embeddings...\")\n",
    "                return normalize(all_embeddings, axis=1)\n",
    "            except ValueError as e:\n",
    "                print(f\"  • Normalization error: {e}\")\n",
    "                print(f\"  • Array shape: {all_embeddings.shape}\")\n",
    "                return all_embeddings\n",
    "        return all_embeddings\n",
    "\n",
    "def analyze_with_bertopic(texts, embeddings):\n",
    "    \"\"\"\n",
    "    Perform BERTopic analysis with enhanced settings.\n",
    "    \n",
    "    Steps:\n",
    "    1. Initialize BERTopic with custom parameters\n",
    "    2. Fit model to embeddings\n",
    "    3. Reduce and refine topics\n",
    "    4. Generate visualizations\n",
    "    \n",
    "    Args:\n",
    "        texts (list): Processed text strings\n",
    "        embeddings (np.ndarray): Corresponding embeddings\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (BERTopic model, topic assignments)\n",
    "    \"\"\"\n",
    "    logging.info(\"\\n Analyzing topics with BERTopic...\")\n",
    "    \n",
    "    # Initialize with custom parameters\n",
    "    logging.info(\"  • Configuring BERTopic model...\")\n",
    "    # Vectorizer settings\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        min_df=5,      # Ignore rare terms\n",
    "        max_df=0.8,     # Ignore overly common terms\n",
    "        ngram_range=(1, 2)  # Include bigrams\n",
    "    )\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=None,  # Using pre-computed embeddings\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        min_topic_size=50,     # Minimum tweets per topic\n",
    "        nr_topics=\"auto\",      # Automatic topic count\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    logging.info(\"  • Fitting model to embeddings...\")\n",
    "    topics, probs = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "    # Reduce topics\n",
    "    logging.info(\"\\n Refining topics...\")\n",
    "    logging.info(\"  • Reducing to ~20 key topics...\")\n",
    "    topic_model.reduce_topics(texts, nr_topics=20)\n",
    "    updated_topics = topic_model.topics_\n",
    "\n",
    "    # Dynamic labeling\n",
    "    logging.info(\"  • Updating topic labels with bigrams...\")\n",
    "    topic_model.update_topics(texts, n_gram_range=(1, 2))\n",
    "\n",
    "    # Show topic info\n",
    "    logging.info(\"\\n=== TOPIC SUMMARY ===\")\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    print(topic_info.head(10))\n",
    "\n",
    "    # Visualize Topic Landscape\n",
    "    fig_topics = topic_model.visualize_topics()\n",
    "    fig_topics.write_html(\"bertopic_visualization.html\")\n",
    "    print(\"  • Topic visualization saved to bertopic_visualization.html\")\n",
    "\n",
    "    # Visualize Topic Frequency Barchart\n",
    "    fig_barchart = topic_model.visualize_barchart()\n",
    "    fig_barchart.show()\n",
    "\n",
    "    # Visualize Hierarchical Topic Tree\n",
    "    fig_hierarchy = topic_model.visualize_hierarchy()\n",
    "    fig_hierarchy.show()  \n",
    "    \n",
    "    return topic_model, updated_topics\n",
    "\n",
    "def process_tweets(tweet_texts, original_df=None):\n",
    "    \"\"\"\n",
    "    Complete tweet processing pipeline with datetime preservation.\n",
    "    \n",
    "    Args:\n",
    "        tweet_texts (list): Pre-processed tweet texts\n",
    "        original_df (pd.DataFrame): Original DataFrame containing metadata\n",
    "                \n",
    "    Returns:\n",
    "        tuple: (results DataFrame, BERTopic model)\n",
    "    \"\"\"\n",
    "    # Text cleaning\n",
    "    logging.info(\"\\n Cleaning and preprocessing tweets...\")\n",
    "    cleaned_texts = [clean_text(t) for t in tweet_texts]\n",
    "    \n",
    "    # Batch processing\n",
    "    processed_texts = []\n",
    "    for i in tqdm(range(0, len(cleaned_texts), MAX_BATCH_SIZE), \n",
    "              desc=\"Preprocessing\", unit=\"batch\"):\n",
    "        batch = cleaned_texts[i:i + MAX_BATCH_SIZE]\n",
    "        processed_texts.extend(preprocess_batch(batch))\n",
    "    \n",
    "    # Filter empty texts and preserve original indices\n",
    "    valid_indices = [i for i, text in enumerate(processed_texts) if text.strip()]\n",
    "    processed_texts = [processed_texts[i] for i in valid_indices]\n",
    "    tweet_texts = [tweet_texts[i] for i in valid_indices]\n",
    "    \n",
    "    # Create results DataFrame with datetime if available\n",
    "    result_data = {\n",
    "        'Cleaned Tweet': tweet_texts,\n",
    "        'Processed Tweet': processed_texts,\n",
    "        'Topic': None  # Will be filled after BERTopic\n",
    "    }\n",
    "    \n",
    "    # Add datetime from original DataFrame if available\n",
    "    if original_df is not None and 'Tweet Datetime Posted' in original_df.columns:\n",
    "        result_data['Tweet Datetime Posted'] = original_df.iloc[valid_indices]['Tweet Datetime Posted'].values\n",
    "    \n",
    "    df = pd.DataFrame(result_data)\n",
    "    \n",
    "    # Embedding generation\n",
    "    logging.info(\"\\n Generating BERTweet embeddings...\")\n",
    "    embedder = EmbeddingGenerator()\n",
    "    embeddings = embedder.generate_embeddings(processed_texts)\n",
    "    del embedder\n",
    "    gc.collect()\n",
    "\n",
    "    # BERTopic analysis\n",
    "    topic_model, topics = analyze_with_bertopic(processed_texts, embeddings)\n",
    "    df['Topic'] = topics  # Add topics to DataFrame\n",
    "    \n",
    "    df.to_csv(\"bertopic_results.csv\", index=False)\n",
    "    return df, topic_model\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main execution pipeline for travel tweet topic modeling.\n",
    "    \"\"\"\n",
    "    tweet_texts = df_filtered[\"Cleaned Tweet\"].astype(str).tolist()\n",
    "   \n",
    "    try:\n",
    "        print(\"\\n=== BERTOPIC ANALYSIS PIPELINE ===\")\n",
    "        df_result, topic_model = process_tweets(tweet_texts, original_df=df_filtered)\n",
    "        \n",
    "        # Save the model and results for the next notebook\n",
    "        topic_model.save(\"bertopic_model\")\n",
    "        df_result.to_pickle(\"bertopic_results.pkl\")\n",
    "        \n",
    "        print(\"\\nBERTopic analysis complete. Results saved to:\")\n",
    "        print(\"- bertopic_model (BERTopic model)\")\n",
    "        print(\"- bertopic_results.pkl (processed results)\")\n",
    "        print(\"- bertopic_visualization.html (interactive topics)\")\n",
    "        print(\"- bertopic_results.csv (complete analysis)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n=== ERROR ===\")\n",
    "        print(f\"Pipeline failed: {str(e)}\")\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafc1c39-aa43-4271-8ed0-bd25efae2e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BERTopic Analysis Pipeline (with Progress Bar Fixes)\n",
    "\n",
    "This script performs advanced topic modeling using BERTopic, including:\n",
    "- Text cleaning and preprocessing\n",
    "- BERTweet embeddings generation\n",
    "- Dynamic topic modeling with BERTopic\n",
    "- Visualization\n",
    "- Trend analysis over time\n",
    "\n",
    "Key Improvements:\n",
    "- Guaranteed tqdm progress bar visibility\n",
    "- Better batch processing feedback\n",
    "- Environment-aware progress displays\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import normalize\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Setup logging for better tracking and error handling\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration\n",
    "MAX_BATCH_SIZE = 3000  # Max texts per preprocessing batch for memory efficiency\n",
    "EMBEDDING_BATCH_SIZE = 32  # Batch size for embedding generation\n",
    "\n",
    "# Initialize spaCy with optimized settings\n",
    "print(\"Loading NLP models...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\", \"tagger\"]) \n",
    "\n",
    "# Custom stopwords for travel domain\n",
    "TRAVEL_STOPWORDS = set([\n",
    "    \"the\", \"a\", \"an\", \"and\", \"in\", \"on\", \"for\", \"to\", \"with\", \"at\", \"by\", \"of\", \n",
    "    \"from\", \"as\", \"it\", \"is\", \"that\", \"this\", \"you\", \"are\", \"i\", \"be\", \"have\"\n",
    "])\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Robust text cleaning for tweets\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "def preprocess_batch(texts):\n",
    "    \"\"\"Process batch of tweets with spaCy\"\"\"\n",
    "    processed_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=1000):\n",
    "        tokens = [token.lemma_.lower() for token in doc \n",
    "                 if token.lemma_.lower() not in TRAVEL_STOPWORDS]\n",
    "        processed_texts.append(\" \".join(tokens) if tokens else \"\")\n",
    "    return processed_texts\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"BERTweet Embedding Generator with improved progress tracking\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"  • Using device: {self.device}\")\n",
    "        print(\"  • Loading BERTweet model...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "        self.model = AutoModel.from_pretrained(\"vinai/bertweet-base\").to(self.device).eval()\n",
    "        print(\"  • Model loaded successfully\")\n",
    "\n",
    "    def generate_embeddings(self, texts):\n",
    "        \"\"\"Generate embeddings with guaranteed progress bars\"\"\"\n",
    "        embeddings = []\n",
    "        logging.info(f\"  • Generating embeddings for {len(texts)} texts...\")\n",
    "        \n",
    "        # Force progress bar visibility\n",
    "        pbar = tqdm(range(0, len(texts), EMBEDDING_BATCH_SIZE),\n",
    "                   desc=\"Generating Embeddings\",\n",
    "                   unit=\"batch\",\n",
    "                   file=sys.stdout,\n",
    "                   mininterval=1)\n",
    "        \n",
    "        for i in pbar:\n",
    "            batch = texts[i:i + EMBEDDING_BATCH_SIZE]\n",
    "            with torch.no_grad():\n",
    "                inputs = self.tokenizer(\n",
    "                    batch, \n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    max_length=64\n",
    "                ).to(self.device)\n",
    "                outputs = self.model(**inputs)\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.append(batch_embeddings)\n",
    "                pbar.set_postfix({\"Batch\": f\"{i//EMBEDDING_BATCH_SIZE + 1}/{(len(texts)//EMBEDDING_BATCH_SIZE)+1}\"})\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        if not embeddings:\n",
    "            return np.array([])\n",
    "        \n",
    "        all_embeddings = np.vstack(embeddings)\n",
    "        return normalize(all_embeddings, axis=1) if all_embeddings.size > 0 else all_embeddings\n",
    "\n",
    "def analyze_with_bertopic(texts, embeddings):\n",
    "    \"\"\"BERTopic analysis with better logging\"\"\"\n",
    "    logging.info(\"\\nAnalyzing topics with BERTopic...\")\n",
    "    \n",
    "    vectorizer_model = CountVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        min_df=5,\n",
    "        max_df=0.8,\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=None,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        min_topic_size=50,\n",
    "        nr_topics=\"auto\",\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    topics, probs = topic_model.fit_transform(texts, embeddings)\n",
    "    topic_model.reduce_topics(texts, nr_topics=20)\n",
    "    topic_model.update_topics(texts, n_gram_range=(1, 2))\n",
    "    \n",
    "    logging.info(\"\\n=== TOPIC SUMMARY ===\")\n",
    "    print(topic_model.get_topic_info().head(10))\n",
    "    \n",
    "    topic_model.visualize_topics().write_html(\"bertopic_visualization.html\")\n",
    "    topic_model.visualize_barchart().show()\n",
    "    topic_model.visualize_hierarchy().show()\n",
    "    \n",
    "    return topic_model, topic_model.topics_\n",
    "\n",
    "def process_tweets(tweet_texts, original_df=None):\n",
    "    \"\"\"Complete processing pipeline with improved progress tracking\"\"\"\n",
    "    logging.info(\"\\nCleaning and preprocessing tweets...\")\n",
    "    \n",
    "    # Text cleaning with progress\n",
    "    cleaned_texts = []\n",
    "    with tqdm(tweet_texts, desc=\"Cleaning\", unit=\"tweet\", file=sys.stdout) as pbar:\n",
    "        cleaned_texts = [clean_text(t) for t in pbar]\n",
    "    \n",
    "    # Batch processing with enhanced progress\n",
    "    processed_texts = []\n",
    "    pbar = tqdm(range(0, len(cleaned_texts), MAX_BATCH_SIZE),\n",
    "              desc=\"Preprocessing\",\n",
    "              unit=\"batch\",\n",
    "              file=sys.stdout)\n",
    "    \n",
    "    for i in pbar:\n",
    "        batch = cleaned_texts[i:i + MAX_BATCH_SIZE]\n",
    "        processed_texts.extend(preprocess_batch(batch))\n",
    "        pbar.set_postfix({\"Processed\": f\"{min(i + MAX_BATCH_SIZE, len(cleaned_texts))}/{len(cleaned_texts)}\"})\n",
    "    pbar.close()\n",
    "    \n",
    "    # Filter empty texts\n",
    "    valid_indices = [i for i, text in enumerate(processed_texts) if text.strip()]\n",
    "    processed_texts = [processed_texts[i] for i in valid_indices]\n",
    "    tweet_texts = [tweet_texts[i] for i in valid_indices]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    result_data = {\n",
    "        'Cleaned Tweet': tweet_texts,\n",
    "        'Processed Tweet': processed_texts,\n",
    "        'Topic': None\n",
    "    }\n",
    "    \n",
    "    if original_df is not None and 'Tweet Datetime Posted' in original_df.columns:\n",
    "        result_data['Tweet Datetime Posted'] = original_df.iloc[valid_indices]['Tweet Datetime Posted'].values\n",
    "    \n",
    "    df = pd.DataFrame(result_data)\n",
    "    \n",
    "    # Embedding generation\n",
    "    logging.info(\"\\nGenerating BERTweet embeddings...\")\n",
    "    embedder = EmbeddingGenerator()\n",
    "    embeddings = embedder.generate_embeddings(processed_texts)\n",
    "    del embedder\n",
    "    gc.collect()\n",
    "\n",
    "    # BERTopic analysis\n",
    "    topic_model, topics = analyze_with_bertopic(processed_texts, embeddings)\n",
    "    df['Topic'] = topics\n",
    "    \n",
    "    df.to_csv(\"bertopic_results.csv\", index=False)\n",
    "    return df, topic_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"\\n=== BERTOPIC ANALYSIS PIPELINE ===\")\n",
    "        df_result, topic_model = process_tweets(df_filtered[\"Cleaned Tweet\"].astype(str).tolist(), \n",
    "                                original_df=df_filtered)\n",
    "        \n",
    "        topic_model.save(\"bertopic_model\")\n",
    "        df_result.to_pickle(\"bertopic_results.pkl\")\n",
    "        \n",
    "        print(\"\\nResults saved to:\")\n",
    "        print(\"- bertopic_model\")\n",
    "        print(\"- bertopic_results.pkl\")\n",
    "        print(\"- bertopic_visualization.html\")\n",
    "        print(\"- bertopic_results.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: {str(e)}\")\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7ae61-87eb-48b5-a974-33f892aee6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Load the saved model and results\n",
    "print(\"Loading saved model and results...\")\n",
    "topic_model = BERTopic.load(\"bertopic_model\")\n",
    "df_result = pd.read_pickle(\"bertopic_results.pkl\")\n",
    "\n",
    "# Sample Tweets\n",
    "def visualize_topics_with_examples(topic_model, df_results):\n",
    "    # Get topic info\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    # Create a styled DataFrame for display\n",
    "    display_df = topic_info[['Topic', 'Count', 'Name']].copy()\n",
    "    display_df['Sample Tweet'] = \"\"\n",
    "    \n",
    "    # Get sample tweets for each topic\n",
    "    for topic in display_df['Topic']:\n",
    "        if topic != -1:  # Skip noise topic\n",
    "            topic_tweets = df_results[df_results['Topic'] == topic]\n",
    "            sample_size = min(3, len(topic_tweets))\n",
    "            sample_tweets = topic_tweets['Cleaned Tweet'].sample(sample_size)\n",
    "            display_df.loc[display_df['Topic'] == topic, 'Sample Tweet'] = \"<br>• \" + \"<br>• \".join(sample_tweets)\n",
    "    \n",
    "    # Create standalone bar chart\n",
    "    bar_chart = go.Figure(\n",
    "        data=[go.Bar(x=display_df['Name'], y=display_df['Count'], marker_color=px.colors.qualitative.Plotly)],\n",
    "        layout=go.Layout(\n",
    "            title=\"Topic Distribution\",\n",
    "            xaxis=dict(title=\"Topic Name\"),\n",
    "            yaxis=dict(title=\"Tweet Count\"),\n",
    "            height=600\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create standalone table with sample tweets\n",
    "    table = go.Figure(\n",
    "        data=[go.Table(\n",
    "            header=dict(\n",
    "                values=['Topic', 'Name', 'Sample Tweet'], \n",
    "                align='left'\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=[display_df['Topic'], display_df['Name'], display_df['Sample Tweet']],\n",
    "                align='left', \n",
    "                height=40\n",
    "            ),\n",
    "            columnwidth=[0.1, 0.2, 0.7]  # Adjust column widths: Topic is smaller, Name a bit smaller, Sample Tweet bigger\n",
    "        )],\n",
    "        layout=go.Layout(\n",
    "            title=\"Topic Details with Sample Tweets\",\n",
    "            height=800\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return bar_chart, table\n",
    "\n",
    "#Timeline of Topics with News Events\n",
    "def create_topic_timeline(df_result):\n",
    "    # Ensure 'Tweet Datetime Posted' exists in df_result by copying from df_filtered\n",
    "    if 'Tweet Datetime Posted' not in df_result.columns:\n",
    "        df_result['Tweet Datetime Posted'] = df_result['Tweet Datetime Posted'].copy()\n",
    "\n",
    "    if 'Tweet Datetime Posted' in df_result.columns and df_result['Tweet Datetime Posted'].notnull().any():\n",
    "        try:\n",
    "            # Convert to datetime and extract month-year\n",
    "            df_result['date'] = pd.to_datetime(df_result['Tweet Datetime Posted']).dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "            # Group by month and topic\n",
    "            topic_trends = df_result.groupby(['date', 'Topic']).size().reset_index(name='count')\n",
    "\n",
    "            # Normalize to get percentage/probability\n",
    "            total_tweets = topic_trends.groupby('date')['count'].transform('sum')\n",
    "            topic_trends['probability'] = topic_trends['count'] / total_tweets\n",
    "\n",
    "            # Remove invalid topics (e.g., topic -1 for noise)\n",
    "            topic_trends = topic_trends[topic_trends['Topic'] != -1]\n",
    "\n",
    "            # Create interactive line plot for topic trends\n",
    "            fig = px.line(\n",
    "                topic_trends,\n",
    "                x='date',\n",
    "                y='probability',\n",
    "                color='Topic',\n",
    "                title='Topic Trends Over Time',\n",
    "                labels={'probability': 'Percentage of Tweets', 'Topic': 'Topic ID'},\n",
    "                hover_data={'Topic': True, 'probability': ':.2f', 'count': True},\n",
    "                width=1000,\n",
    "                height=600\n",
    "            )\n",
    "\n",
    "            # Layout tweaks\n",
    "            fig.update_layout(\n",
    "                xaxis_title='Date',\n",
    "                yaxis_title='Percentage of Tweets',\n",
    "                hovermode='x unified',\n",
    "                legend_title_text='Topic ID'\n",
    "            )\n",
    "\n",
    "            # Optional annotation \n",
    "            #if len(topic_trends['date'].unique()) > 6:\n",
    "            #    fig.add_annotation(\n",
    "            #        x=topic_trends['date'].median(),\n",
    "            #        y=topic_trends['probability'].max() * 0.9,\n",
    "            #        text=\"Peak Discussion Period\",\n",
    "            #        showarrow=True,\n",
    "            #        arrowhead=1\n",
    "            #    )\n",
    "\n",
    "            # Save the figure as HTML\n",
    "            fig.write_html(\"bertopic_trends_over_time.html\")\n",
    "            \n",
    "            return fig\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create trends visualization: {str(e)}\")\n",
    "    else:\n",
    "        print(\"No valid datetime information available for trend analysis\")\n",
    "        return None\n",
    "\n",
    "# Run visualizations\n",
    "print(\"Creating visualizations...\")\n",
    "topic_viz_bar, topic_viz_table = visualize_topics_with_examples(topic_model, df_result)\n",
    "timeline_viz = create_topic_timeline(df_result)\n",
    "\n",
    "# Display results\n",
    "print(\"Displaying results...\")\n",
    "topic_viz_bar.show()\n",
    "topic_viz_table.show()\n",
    "if timeline_viz is not None:  # Only show if timeline was created\n",
    "    timeline_viz.show()\n",
    "\n",
    "print(\"BERTopic visualization complete.\")\n",
    "print(\"Results saved to: bertopic_trends_over_time.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
